idea_gw_hosts: "*"
mtls_mode: STRICT

istio_injection: enabled
tls:
  commonName: "127.0.0.1"

#Auth
#enable/disable Auth configuration
#e.g.: helm install idea4rc-capsule idea4rc-capsule/ --set auth_enable=False
auth_enable: False
#jwt_issuer: "https://keycloak.idea.lst.tfo.upm.es/realms/IDEA4RC"
#jwksUri: "https://keycloak.idea.lst.tfo.upm.es/realms/IDEA4RC/protocol/openid-connect/certs"

##multiple roles for authentication can be declared here
#auth_policies:
#  medic:
#    policy_name: require-jwt-medic
#    methods: "POST"
#    roles: "Medic"
#    namespace: datamesh
#  patient:
#    policy_name: require-jwt
#    methods: "GET"
#    roles: "Patient"
#    namespace: datamesh

#Custom storage class for k8s. 
#enables persistency for the various DB instances of the capsule across capsule upgrades
#this can lead to data inconsistecy and other issues. Deleting the capsule will delete the volumes, too.
idea4rc_sc:
  enabled: True
  is_default: "true"
  name: idea4rc-hostpath
  provisioner: microk8s.io/hostpath
  reclaimpolicy: Retain
  volumebindingmode: Immediate

#FHIR server configuration
fhir:
  service:
    name: fhir-svc
    type: ClusterIP
    port: 8080
    target_port: 8080
  metrics:
    serviceMonitor:
      enabled: false
    additionalLabels: {}
    service:
      port: 8081
  pod_disruption_budget:
    enabled: false
    minAvailable: 1
    maxUnavailable: ""
  server:
    name: fhir-server
    namespace: datamesh
    replicas: 1
    image_pull_secrets: []
    pod_security_context: {}
    security_context:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - ALL
      readOnlyRootFilesystem: true
      runAsNonRoot: true
      runAsUser: 65532
      runAsGroup: 65532
      privileged: false
      seccompProfile:
        type: RuntimeDefault
    container_name: fhir-server
    image:
      registry: docker.io
      repository: hapiproject/hapi
      tag: "v6.10.1@sha256:4eac1b3481180b028616d1fab7e657e368538063d75f7ed3be2032e34c657dd4"
      pull_policy: IfNotPresent
    startup_probe:
      httpGet:
        path: /readyz
        port: http
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 30
    liveness_probe:
      httpGet:
        path: /livez
        port: http
      failureThreshold: 5
      initialDelaySeconds: 30
      periodSeconds: 20
      successThreshold: 1
      timeoutSeconds: 30
    readiness_probe:
      httpGet:
        path: /readyz
        port: http
      failureThreshold: 5
      initialDelaySeconds: 30
      periodSeconds: 20
      successThreshold: 1
      timeoutSeconds: 20
    customConfigPath: /app/config
    customConfigName: application-extra.yaml
    resources:
      {}
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi
    spring:
      spring_pgsql_driver: org.postgresql.Driver
      fhir_hibernate_dialect: ca.uhn.fhir.jpa.model.dialect.HapiFhirPostgres94Dialect
      #the following params are manually set in the corresponding template due to parsing issues with helm
      #use_apache_addr_strategy: "true"
      #probe_add_paths: "true"
      #mgmt_srv_port: "8081"
      extra_config: ""
      extra_env: ""
    node_selector: {}
    affinity: {}
    tolerations: []
    topology_spread_constraints: []
  db:
    name: fhir-postgres-db
    replicas: 1
    image: postgres:13-alpine
    port: 5432
    db_name: idea4rc
    service:
      name: fhir-postgres-db
      port: 5432
      target_port: 5432
      type: ClusterIP

etl:
  namespace: datamesh
  server:
    name: eng-etl
    image: "ghcr.io/idea4rc/etl@sha256:c96078fcc959644319aa6a0839a78a0e273ccbe5a4131269f383b133c8ca9183"
    port: 4001
    #proxy: "false"
    server_dns: "https://"
    hibernate_dialect: org.hibernate.dialect.PostgreSQLDialect
    spring_driver: org.postgresql.Driver
  db:
    name: etl
    replicas: 1
    image: postgres:16.3-bullseye
    port: 5432
    #db_name: mad
    service:
      name: etl-postgres-db
      port: 5432
      target_port: 5432
      type: ClusterIP
  service:
    name: etl-svc
    type: ClusterIP
    port: 4001
    target_port: 4001

aerospike:
  name: aerospike
  namespace: datamesh
  server:
    keystore_namespace: idea4rc
    image: "aerospike/aerospike-server:7.1"
    port: 3000
    mtls_mode: PERMISSIVE
  service:
    name: aerospike-svc
    type: ClusterIP
    port: 3000
    target_port: 3000

dataextraction:
  name: dataextraction
  namespace: datamesh
  server:
    image: ghcr.io/idea4rc/idea4rc_ai_dataextractionjobs@sha256:aebb2989b5ab2348410d235926bb78508e14844037b00ae7e7430bc76720cd60
    replicas: 1
    port: 5000
    dropdb: "False"
    swagger_port: 8000
    #this swagger_url is nasty and needs a rework
    #swagger_url: "http://217.172.12.141/dataext/api/doc/swagger.json"
  service:
    name: dataextract-svc
    type: ClusterIP
    port: 5000
    target_port: 5000
    swagger_port: 8000
    swagger_target_port: 8000
  db:
    name: dataextract-postgres-db
    replicas: 1
    image: postgres:16.3-bullseye
    port: 5432
    db_name: mad
    service:
      name: dataextract-postgres-db
      port: 5432
      target_port: 5432
      type: ClusterIP

omop:
  namespace: datamesh
  app: omop-postgres
  cdm:
    name: omop-cdm-deployment
    replicas: "1"
    image: ghcr.io/idea4rc/omop-deploy@sha256:2b873c3c450193349d91957c66d5ced35d13cd271ebc660facca9af7134449b4
    pullpolicy: IfNotPresent
    port: 5432
  vocab:
    name: populate-db-job
    ttl: 120
    image: ghcr.io/idea4rc/omop-vocab-uploader@sha256:9c38c4a3e95c08933c63f1e3afe9cd41f300352a223c7fed04db87fd0d078900
    pullpolicy: IfNotPresent
  db:
    name: omopdb
    schema: omopcdm
  vocabJob: true
  service:
    name: omop-postgres-service
    port: 5432
    target_port: 5432
    type: ClusterIP

workbench:
  name: capsule-workbench
  namespace: datamesh
  replicas: 1
  image: ghcr.io/idea4rc/capsule-workbench@sha256:5de9ca7610fd4497b26ebef08a82e6b42545303ffc81fa4dccc5bbfcfa4a9f86
  port: 80
  service:
    name: workbench-svc
    port: 80
    target_port: 80
    type: ClusterIP

revproxy:
  namespace: datamesh
  app: revproxy
  server:
    name: revproxy
    replicas: 1
    image: httpd:2.4.61
    port: 80
    capsule_public_host: "127.0.0.1"
  service:
    name: revproxy-svc
    port: 80
    target_port: 80
    type: ClusterIP

rabbitmq:
  name: rabbitmq
  namespace: datamesh
  replicas: 1
  image: rabbitmq:3-management
  port:
    rabbit_port: 5672
    management_port: 15672
  service:
    name: rabbitmq-service
    rabbit_port: 5672
    management_port: 15672

ohdsi:
  name: ohdsi-api
  namespace: datamesh
  replicas: "1"
  image: harbor2.vantage6.ai/infrastructure/ohdsi-api@sha256:aec7c1d7cbaf82cf3af56a33c98610351bd7eb2b490daf61dc4f80ac76fb02ad
  imagePullPolicy: IfNotPresent
  port: 8000
  service:
    name: ohdsi-api
    port: 8000
    type: ClusterIP

celery:
  namespace: datamesh
  db:
    name: celery-db
    database: ohdsi
    replicas: 1
    image: postgres:16.3-bullseye
    port: 5432
    #DB usr/pwd specified here as a workaround for helm being 
    #unable to provide a way to use a secret inside a string.
    #The lookup technique used proviously works only if a 
    #secret is already deployed.
    #dbuser: ohdsiuser
    #dbpasswd: hjds9a8d9h.dsa
    service:
      name: celery-db-svc
      port: 5432
      target_port: 5432
  worker:
    name: celery-worker
    replicas: 1
    image: harbor2.vantage6.ai/infrastructure/ohdsi-api@sha256:aec7c1d7cbaf82cf3af56a33c98610351bd7eb2b490daf61dc4f80ac76fb02ad
    service:
      name: celery-svc
      port: 5672
      target_port: 5672

ideadb:
  namespace: datamesh
  server:
    name: ideadb
    image: bitnami/mariadb:11.4.2
    imagePullPolicy: IfNotPresent
    config: ""
    persistence:
      enabled: false
      storageClass: generic
      accessMode: ReadWriteOnce
      size: 8Gi
  db:
    name: ideadb
    port: 3306
  service:
    name: ideadb-svc
    port: 3306
    target_port: mysql