# This is the default namespace for the capsule. Please note that this variable *DOES NOT SET* the namespace of the individual components. You'll have to change/override that for each component that you whish to deploy those in a specific namespace.
defaultNamespace: datamesh

# This is the IP/Domain Name used to expose the capsule, should be overridden at install/upgrade time with the desired IP using --set capsulePublicHost=(MY CAPSULE IP)
capsulePublicHost: "127.0.0.1"

# ISTIO configuration is defined here
istio:
  ideaGwHosts: "*"
  mtlsMode: STRICT
  istioInjection: enabled
  tls:
    commonName: "127.0.0.1"

# Enable/disable Auth configuration and configure multiple roles for authentication.
auth:
  enable: False
  jwtIssuer: "https://keycloak.idea.lst.tfo.upm.es/realms/IDEA4RC"
  jwksUri: "https://keycloak.idea.lst.tfo.upm.es/realms/IDEA4RC/protocol/openid-connect/certs"
  policies:
    medic:
      policy_name: require-jwt-medic
      methods: "POST"
      roles: "Medic"
      namespace: datamesh
    patient:
      policy_name: require-jwt
      methods: "GET"
      roles: "Patient"
      namespace: datamesh

# Hostpath-based custom storage class for k8s. This enables persistency for the various DB instances of the capsule across upgrades. This is useful in certain cases where we want to avoid data loss, for eg.: across restarts of the guest machine or when restarting services. This can lead to data inconsistecy and other issues and it's not the inteded way of using the capsule, which should be recreated every time the source dataset changes or when upgrading. Please note that deleting the capsule using "helm delete" will delete the volumes, too.
idea4rcStorageClass:
  name: idea4rc-hostpath
  enabled: true
  isDefault: "true"
  provisioner: microk8s.io/hostpath
  reclaimpolicy: Retain
  volumebindingmode: Immediate

# ETL to FHIR configuration
etl:
  etl:
    name: etl-fhir
    namespace: datamesh
    server:
      name: etl-fhir
      image: ghcr.io/idea4rc/etl:2.0
      parserSeparator: ","
    db:
      image: postgres:16.3-bullseye
    virtualService:
      enabled: true
  aerospike:
    name: aerospike
    namespace: datamesh
    server:
      keystoreNamespace: idea4rc
      image: "aerospike/aerospike-server:7.1"
      mtlsMode: PERMISSIVE

## FHIR data server configuration
fhirDataServer:
  service:
    name: fhir-data-svc
  server:
    name: fhir-data-server
    namespace: datamesh
    image:
      registry: ghcr.io
      repository: idea4rc/fhir-data-server
      tag: "1.0"
      pull_policy: IfNotPresent
    configmap_name: fhir-data-application-cm
  db:
    name: fhir-data-postgres-db
    image: postgres:13-alpine
    dbName: idea4rc
    service:
      name: fhir-data-postgres-db
      port: 5432
    pvc:
      name: fhir-dataserver-pg-pv-claim
    secret:
      name: fhir-dataserver-pg-secret
  virtualService:
    enabled: false

## NLP
nlp:
  nlpingestion:
    namespace: datamesh
    statusweb:
      name: nlp-ingestion-statusweb
      namespace: datamesh
      image: ghcr.io/unai-zulaika/idea4rc-nlp-ingestion-status-web:0.2.3
      etlHost: "etl-svc.datamesh.svc.cluster.local:4001"
      resultsUIHost: "nlpresultsui.datamesh.svc.cluster.local"
      service:
        name: nlpstatus
    api:
      namespace: datamesh
      image: ghcr.io/unai-zulaika/idea4rc-nlp-ingestion-api:0.2
      service:
        name: nlpapi
      storageClass:
        name: idea4rc-hostpath
    resultsui:
      namespace: datamesh
      image: ghcr.io/unai-zulaika/idea4rc-nlp-ingestion-results-ui:0.2.2
      baseURL: "/nlp/results"
      service:
        name: nlpresultsui
      storageClass:
        name: idea4rc-hostpath
    
## OMOP Database
omop:
  namespace: datamesh
  app: omop-postgres
  cdm:
    name: omop-cdm-deployment
    image: ghcr.io/idea4rc/omop-deploy@sha256:2b873c3c450193349d91957c66d5ced35d13cd271ebc660facca9af7134449b4
  vocab:
    name: populate-db-job
    ttl: 120
    image: ghcr.io/idea4rc/omop-vocab-uploader@sha256:dc81445a11146ed9bd2c12ceab940c6a4e6b4b9388c1fcb165d49fa9f0d976f8
  db:
    name: omopdb
    schema: omopcdm_synthetic
    result_schema: results_synthetic
  service:
    name: omop-postgres-service
  vocabJob: false

## Capsule Workbench
workbench:
  name: capsule-workbench
  namespace: datamesh
  image: ghcr.io/idea4rc/capsule-workbench@sha256:5de9ca7610fd4497b26ebef08a82e6b42545303ffc81fa4dccc5bbfcfa4a9f86

## Apache Reverse Proxy for those WebApps that have issues with 
## ISTIO's reverse proxy functionality
revproxy:
  namespace: datamesh
  app: revproxy
  server:
    name: revproxy
    replicas: 1
    image: httpd:2.4.61
    port: 80
  service:
    name: revproxy-svc
    port: 80
    targetPort: 80
    type: ClusterIP
  virtualService:
    enabled: "false"

## Cohort Builder Query Executor
fcbexec:
  name: feasibility-cohort-builder-exec
  namespace: datamesh
  image: "ghcr.io/idea4rc/cohortbuilder-exec@sha256:8984c5f690e65da21d21f5e0168d6c6b5238ed1ead960c58cb7ac79d8c6e01a7"
  port: 3000
  serviceURL: "https://api.fcb.orchestrator.idea.lst.tfo.upm.es"
  fhirServerURL: "http://fhir-data-svc.datamesh.svc.cluster.local" # changed to Internal FHIR server URL, needs testing
  postgress:
    dbName: etl
  service:
    name: fcbexec-svc
    port: 3000
    targetPort: 3000
  # all these values are placeholders
  kafka:
    #consumerId and clientId should be unique for each capsule
    consumerId: capsule_1_consumer
    fhirTopic: fhir_queries
    queryResultTopic: fhir_queries_results
    sqlTopic: sql_queries
    sqlResultTopic: sql_queries_results
    host: orchestrator.idea.lst.tfo.upm.es
    port: "9093"
    clientId: capsule_test
  keyCloak:
    URL:  /realms/idea4rc/protocol/openid-connect/token/introspect
    port: "443"
    host: https://idea4rc-keykloak.development-iti.com/auth
    clientId: placeholder
    clientSecret: placeholder

## Vantage6 Node configuration
v6node:
  namespace: datamesh
  node:
    name: default-capsule-node # this name needs to be unique across all capsules
    createSecrets: false # we want to use our secret instead because this chart handles the omop auth data
    apiKey: put-your-api-key-here
    server:
      url: "https://vantage6-core.orchestrator.idea.lst.tfo.upm.es"
      port: 443
      path: /server
    image: harbor2.vantage6.ai/infrastructure/node:5.0.0a43
    proxyPort: 7654
    k8sNodeName: idea4rc  # used for affinity with the k8s node that has access to the file based storage
    keycloakUrl: "https://vantage6-auth.orchestrator.idea.lst.tfo.upm.es/"
    keycloakRealm: vantage6
    logging:
      level: DEBUG
      loggers:
      - level: warning
        name: urllib3
      - level: warning
        name: socketIO-client
      - level: debug
        name: socketio.server
      - level: debug
        name: engineio.server
      - level: debug
        name: sqlalchemy.engine
      - level: warning
        name: kubernetes.client.rest
    persistence:
      tasks:
        storageClass: idea4rc-hostpath
        size: 2Gi
        # the following path needs to be created beforehand and needs the correct permissions
        hostPath: /mnt/vantage6/tasks
      database:
        storageClass: idea4rc-hostpath
        size: 1Gi
    databases:
      #fileBased: []
      # this file absed DB conf is a placeholder, wating for a path to the v6 node to remove it in not actually required 
      fileBased:
      - name: default
        uri: /mnt/test.csv
        type: csv
        volumePath: /mnt/
        originalName: test.csv
      serviceBased:
      - name: postgres
        # this URI points to OMOP 
        # uri: postgres://vantage6:vantage6@vantage6-vantage6-postgres:5432/vantage6
        uri: jdbc:postgresql://omop-postgres-service.datamesh.svc.cluster.local:5432/vantage6
        type: other
    taskNamespace: v6-tasks
